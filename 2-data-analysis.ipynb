{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model training & Data Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bigml.api\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instantiate BigMl - need BigML's project id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "api = bigml.api.BigML(project='project/5db1644859f5c33b3c00076c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Sources files\n",
    "train_full_source = api.create_source('./full_train_edit.csv')\n",
    "test_source = api.create_source('./test_edit.csv')\n",
    "api.ok(test_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "train_full_dataset = api.create_dataset(train_full_source)\n",
    "test_dataset = api.create_dataset(test_source)\n",
    "api.ok(train_full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Splitting train_full in a train dataset and a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = api.create_dataset(\n",
    "    train_full_dataset, {\"name\": \"Train Dataset\",\n",
    "                     \"sample_rate\": 0.8, \"seed\": \"my seed\"})\n",
    "api.ok(train_dataset)\n",
    "test_dataset = api.create_dataset(\n",
    "    train_full_dataset, {\"name\": \"Validation Dataset\",\n",
    "                     \"sample_rate\": 0.8, \"seed\": \"my seed\",\n",
    "                     \"out_of_bag\": True})\n",
    "api.ok(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = api.create_ensemble(train_dataset, {\"objective_field\": \"SeriousDlqin2yrs\"})\n",
    "api.ok(ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching a prediction\n",
    "\n",
    "### making the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = api.create_batch_prediction(ensemble, test_dataset, {\n",
    "    \"name\": \"1st shot\",\n",
    "    \"all_fields\": True,\n",
    "    \"prediction_name\": \"Prediction\",\n",
    "    \"probabilities\": True\n",
    "})\n",
    "api.ok(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### downloading the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./prediction.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.download_batch_prediction('batchprediction/5dc036bc5299632024000e4f', filename=\"./prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.read_csv(\"./prediction.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the prediction's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the confusion (FP, FN, TP, TN)\n",
    "\n",
    "def set_confusion(row):\n",
    "    if (row['SeriousDlqin2yrs'] == row['Prediction'] and row['SeriousDlqin2yrs'] == 0):\n",
    "        return 'TN'\n",
    "    if (row['SeriousDlqin2yrs'] == row['Prediction'] and row['SeriousDlqin2yrs'] == 1):\n",
    "        return 'TP'\n",
    "    if row['SeriousDlqin2yrs'] > row['Prediction']:\n",
    "        return 'FN'\n",
    "    return 'FP'\n",
    "        \n",
    "prediction_df['confusion'] = prediction_df.apply(set_confusion, axis=1)\n",
    "prediction_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting in csv the 100 biggest errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_classified = prediction_df.sort_values(by=['confusion','1 probability'], ascending=[True, True])\n",
    "confusion = prediction_df_classified.groupby(['confusion']).confusion.count()\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation du seuil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creation d'une colone d'erreur en fonction d'un seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_error_with_threshold(row):\n",
    "    if row['1 probability'] > threshold:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "prediction_df['my_prediction'] = prediction_df.apply(set_error_with_threshold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_confusion(row):\n",
    "    if (row['SeriousDlqin2yrs'] == row['my_prediction'] and row['SeriousDlqin2yrs'] == 0):\n",
    "        return 'TN'\n",
    "    if (row['SeriousDlqin2yrs'] == row['my_prediction'] and row['SeriousDlqin2yrs'] == 1):\n",
    "        return 'TP'\n",
    "    if row['SeriousDlqin2yrs'] > row['my_prediction']:\n",
    "        return 'FN'\n",
    "    return 'FP'\n",
    "        \n",
    "prediction_df['confusion'] = prediction_df.apply(set_confusion, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27736, 1668, 347, 249)\n"
     ]
    }
   ],
   "source": [
    "def count_confusion_values(col):\n",
    "    try:\n",
    "        TN = col.value_counts().to_dict()['TN']\n",
    "    except:\n",
    "        TN = 0\n",
    "    try:\n",
    "        FN = col.value_counts().to_dict()['FN']\n",
    "    except:\n",
    "        FN = 0\n",
    "    try:\n",
    "        TP = col.value_counts().to_dict()['TP']\n",
    "    except:\n",
    "        TP = 0\n",
    "    try:\n",
    "        FP = col.value_counts().to_dict()['FP']\n",
    "    except:\n",
    "        FP = 0\n",
    "    return TN, FN, TP, FP\n",
    "confusion_matrix = count_confusion_values(prediction_df['confusion'])\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9573500\n"
     ]
    }
   ],
   "source": [
    "def profit_matrix(confusion_matrix):\n",
    "    TN_cost = 500\n",
    "    FN_cost = -2500\n",
    "    TP_cost = 0\n",
    "    FP_cost = -500\n",
    "    total_cost = (confusion_matrix[0] * TN_cost\n",
    "                 + confusion_matrix[1] * FN_cost\n",
    "                 + confusion_matrix[2] * TP_cost\n",
    "                 +confusion_matrix[3] * FP_cost)\n",
    "    return total_cost\n",
    "\n",
    "profit = profit_matrix(confusion_matrix)\n",
    "print(profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
